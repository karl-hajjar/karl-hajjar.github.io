# About me
I am a 2nd year PhD student at [Laboratoire de Mathématiques d'Orsay](https://www.imo.universite-paris-saclay.fr/fr/) 
under the supervision of [Lénaïc Chizat](https://lchizat.github.io/) and [Christophe Giraud](https://www.imo.universite-paris-saclay.fr/~giraud/), 
working on a mathematical theory of deep neural networks. I mainly study the dynamics induced by gradient methods in the
large-width limit to try to discover interesting properties that would shed light on why such models work so well in practice.
I graduated from Ecole Polytechnique in 2017 and completed my Masters in 2018 at Ecole Normale Supérieure. Before my PhD,
I worked two years in Berlin for Zalando as a research engineer. 

My research interests are around statistics and optimisation, and my goal is to develop a rigorous theory of deep neural 
networks, even if it is at the cost of studying simpler models than the ones actually used in practice, because I believe
it is the only way to extract the essence of what truly works in deep learning from the noise created by the myriad of variations
in architecture and optimisation method. My hope is that in doing so, we might obtain insights on the learning principles
at play in neural networks and therefore develop more performant, robust and less data-hungry networks than current ones.

**Contact:** [karl.hajjar@polytechnique.edu](karl.hajjar@polytechnique.edu)

[CV](data/CV_Karl_Hajjar_phd_june_2021_pdf.pdf)
# Publications 

# Teaching

[//]: # (* * *)


